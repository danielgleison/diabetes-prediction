{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "UNIVERSIDADE ESTADUAL DO CEARÁ \\\n",
    "MESTRADO ACADÊMICO EM CIÊNCIA DA COMPUTAÇÃO \\\n",
    "MINERAÇÃO MASSIVA DE DADOS\n",
    "\n",
    "Daniel Gleison Moreira Lira \\\n",
    "daniel.gleison@aluno.uece.br\n",
    "\n",
    "---\n",
    "# Mecanismo de predição de diagnóstico de diabetes utilizando aprendizado de máquina e processamento distribuído\n",
    "---\n",
    "\n",
    "\n",
    "## Dataset\n",
    "\n",
    "http://archive.ics.uci.edu/ml/datasets/Early+stage+diabetes+risk+prediction+dataset.# \\\n",
    "Date created: 2020-07-12\n",
    "\n",
    "### Associated Tasks:\n",
    "Classification\n",
    "\n",
    "### Predicted attribute:\n",
    "diabetes diagnostic\n",
    "\n",
    "### Number of Instances:\n",
    "520\n",
    "\n",
    "### Number of Attributes:\n",
    "17 (5 Decimal, 3 Integer, 3 String)\n",
    "\n",
    "### Attribute Information:\n",
    "\n",
    "Age 1.20-65 \\\n",
    "Sex 1. Male, 2.Female \\\n",
    "Polyuria 1.Yes, 2.No. \\\n",
    "Polydipsia 1.Yes, 2.No. \\\n",
    "sudden weight loss 1.Yes, 2.No. \\\n",
    "weakness 1.Yes, 2.No. \\\n",
    "Polyphagia 1.Yes, 2.No. \\\n",
    "Genital thrush 1.Yes, 2.No. \\\n",
    "visual blurring 1.Yes, 2.No. \\\n",
    "Itching 1.Yes, 2.No. \\\n",
    "Irritability 1.Yes, 2.No. \\\n",
    "delayed healing 1.Yes, 2.No. \\\n",
    "partial paresis 1.Yes, 2.No. \\\n",
    "muscle stiffness 1.Yes, 2.No. \\\n",
    "Alopecia 1.Yes, 2.No. \\\n",
    "Obesity 1.Yes, 2.No. \\\n",
    "Class 1.Positive, 2.Negative. \n",
    "\n",
    "### Missing Attribute Values: \n",
    "Yes\n",
    "\n",
    "### Class Distribution: \n",
    "2 Classes \\\n",
    "320 Positive and 200 Negative\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spark Lib\n",
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load libraries\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import isnull, when, count, col, regexp_replace\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import DecisionTreeClassifier\n",
    "from pyspark.ml.feature import StringIndexer, VectorIndexer\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.mllib.util import MLUtils\n",
    "\n",
    "from pyspark.ml.feature import StringIndexer, IndexToString\n",
    "from pyspark.ml.feature import VectorAssembler, VectorIndexer\n",
    "\n",
    "\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.ml.classification import MultilayerPerceptronClassifier\n",
    "from pyspark.ml.classification import NaiveBayes\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.classification import LinearSVC, OneVsRest\n",
    "\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator, BinaryClassificationEvaluator\n",
    "\n",
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.mllib.util import MLUtils\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "import time\n",
    "start_time = time.time()\n",
    "%matplotlib inline\n",
    "\n",
    "#Extras\n",
    "##from pyspark.sql.functions import isnull, when, count, col\n",
    "from pyspark.ml.classification import DecisionTreeClassificationModel\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Criação do ambiente Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://10.129.64.20:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.0.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>PredictionDiabetes</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f72e56df588>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Versão do Spark\n",
    "spark = SparkSession.builder \\\n",
    "        .master(\"local[*]\") \\\n",
    "        .appName(\"PredictionDiabetes\") \\\n",
    "        .getOrCreate()\n",
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importação do dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = './data/'\n",
    "data_name = 'diabetes_data_upload.csv'\n",
    "df_original = spark.read.format('csv')\\\n",
    "                   .options(sep=',', header='true',inferschema='true')\\\n",
    "                   .load(data_path+data_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quantidade de linhas: 520\n",
      "Quantidade de colunas: 17\n"
     ]
    }
   ],
   "source": [
    "print(\"Quantidade de linhas:\",df_original.count())\n",
    "print(\"Quantidade de colunas:\",len(df_original.columns))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Análise exploratória dos dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+\n",
      "|count(class)|\n",
      "+------------+\n",
      "|         320|\n",
      "+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_original.createOrReplaceTempView(\"tab_original\")\n",
    "df_sql = spark.sql('select count(class) from tab_original where class = \"Positive\"')\n",
    "df_sql.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+\n",
      "|count(class)|\n",
      "+------------+\n",
      "|         200|\n",
      "+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_sql = spark.sql('select count(class) from tab_original where class = \"Negative\"')\n",
    "df_sql.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_original.groupBy('class').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_original.select(\"*\").toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_original.describe().toPandas().transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_original.show(vertical=True,truncate=False, n=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "data_types = defaultdict(list)\n",
    "for entry in df_original.schema.fields:\n",
    "    data_types[str(entry.dataType)].append(entry.name)\n",
    "data_types"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Identificação de valores ausentes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_original.select([count(when(isnull(c), c)).alias(c) for c in df_original.columns]).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Matrix de correlação"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_original.toPandas().corr()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformação do dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Indexação dos atributos de entrada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_indexado = df_original\n",
    "\n",
    "col = list(range(1, len(df_original.columns) - 1))\n",
    "\n",
    "for x in col:\n",
    "\n",
    "    indexer = StringIndexer(inputCol=df_original.columns[x], outputCol='index_'+df_original.columns[x]).fit(df_original)\n",
    "    df_indexado = indexer.transform(df_indexado)\n",
    "    labelReverse  = IndexToString().setInputCol(df_original.columns[x])\n",
    "df_indexado.toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Indexação da classe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_indexado.select('class').toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_indexado = df_indexado.withColumn('class', regexp_replace('class', 'Positive', '1'))\n",
    "df_indexado = df_indexado.withColumn('class', regexp_replace('class', 'Negative', '0'))\n",
    "df_indexado = df_indexado.withColumn('class',df_indexado['class'].cast('Integer'))\n",
    "df_indexado.select('class').toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_indexado = df_indexado.withColumn('gender', regexp_replace('gender', 'Male', '1'))\n",
    "df_indexado = df_indexado.withColumn('gender', regexp_replace('gender', 'Female', '0'))\n",
    "df_indexado = df_indexado.withColumn('gender',df_indexado['gender'].cast('Integer'))\n",
    "df_indexado.select('gender').toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_indexado = df_indexado.withColumnRenamed ('class', 'label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_indexado.groupBy('label').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "data_types = defaultdict(list)\n",
    "for entry in df_indexado.schema.fields:\n",
    "    data_types[str(entry.dataType)].append(entry.name)\n",
    "data_types"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exclusão de atributos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_indexado.show(vertical=True,truncate=False, n=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_selecionado = df_indexado.drop('Gender',\\\n",
    "                                  'Polyuria',\\\n",
    "                                  'Polydipsia',\\\n",
    "                                  'sudden weight loss',\\\n",
    "                                  'weakness',\\\n",
    "                                  'Polyphagia',\\\n",
    "                                  'Genital thrush',\\\n",
    "                                  'visual blurring',\\\n",
    "                                  'Itching',\\\n",
    "                                  'Irritability',\\\n",
    "                                  'delayed healing',\\\n",
    "                                  'partial paresis',\\\n",
    "                                  'muscle stiffness',\\\n",
    "                                  'Alopecia',\\\n",
    "                                  'Obesity')\\\n",
    "\n",
    "df_selecionado.show(vertical=True,truncate=False, n=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "data_types = defaultdict(list)\n",
    "for entry in df_selecionado.schema.fields:\n",
    "    data_types[str(entry.dataType)].append(entry.name)\n",
    "data_types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_selecionado.filter(df_selecionado['label'] == 1).sort(df_selecionado['label'])\n",
    "df.toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Seleção dos atributos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_selecionado.toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Criação da matrix de classificação"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ignore = ['label']\n",
    "list = [x for x in df_selecionado.columns if x not in ignore]\n",
    "\n",
    "assembler = VectorAssembler(\n",
    "            inputCols= list,\n",
    "            outputCol='features')\n",
    "\n",
    "df_transformado = (assembler.transform(df_selecionado).select('label','features'))\n",
    "df_transformado.show(truncate = False, n = 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Divisão do dataset para treinamento e teste"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sample = 0.7\n",
    "test_sample = 0.3\n",
    "seed = 1234\n",
    "\n",
    "(train, test) = df_transformado.randomSplit([train_sample, test_sample],seed)\n",
    "\n",
    "num_train = df_transformado.count() * train_sample\n",
    "num_test = df_transformado.count() * test_sample\n",
    "\n",
    "print('Percentual da base de treinamento', train_sample*100, '%')\n",
    "print('Percentual da base de teste', test_sample*100, '%')\n",
    "print('Quantidade de registros da base de treinamento:', train.count())\n",
    "print('Quantidade de registros da base de treinamento:', test.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.groupby('label').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.groupby('label').count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Treinamento, teste e avaliação dos modelos de predição"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Tree (DT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Treinamento do modelo de predição\n",
    "start_time = time.time()\n",
    "trainer_dt = DecisionTreeClassifier(featuresCol='features', labelCol='label', predictionCol='prediction', probabilityCol='probability',\\\n",
    "                                 rawPredictionCol='rawPrediction', maxDepth=5, maxBins=32, minInstancesPerNode=1, minInfoGain=0.0,\\\n",
    "                                 maxMemoryInMB=256, cacheNodeIds=False, checkpointInterval=10, impurity='gini', seed=None)\n",
    "model_dt = trainer_dt.fit(train)\n",
    "time_dt_train = time.time() - start_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execução do modelo de predição na base de teste\n",
    "start_time = time.time()\n",
    "result_dt = model_dt.transform(test)\n",
    "time_dt_pred = time.time() - start_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cálculo da acurácia do modelo de predição\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol='label', predictionCol='prediction',\\\n",
    "            metricName='accuracy')\n",
    "accuracy_dt = evaluator.evaluate(result_dt) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cálculo do recall do modelo de predição\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol='label', predictionCol='prediction',\\\n",
    "            metricName='weightedRecall')\n",
    "recall_dt = evaluator.evaluate(result_dt) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cálculo da precisão do modelo de predição\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol='label', predictionCol='prediction',\\\n",
    "            metricName='weightedPrecision')\n",
    "precision_dt = evaluator.evaluate(result_dt) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cálculo da F1 score do modelo de predição\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol='label', predictionCol='prediction',\\\n",
    "            metricName='f1')\n",
    "f1_dt = evaluator.evaluate(result_dt) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Matriz de confusão\n",
    "y_true = result_dt.select(\"label\").toPandas()\n",
    "y_pred = result_dt.select(\"prediction\").toPandas()\n",
    "mc_dt = confusion_matrix(y_true, y_pred, labels = [0,1])\n",
    "tn_dt, fp_dt, fn_dt, tp_dt = confusion_matrix(y_true, y_pred).ravel()\n",
    "print(mc_dt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "group_names = ['Verdadeiro Negativo','Falso Positivo','Falso Negativo','Verdadeiro Positivo']\n",
    "group_counts = ['{0:0.0f}'.format(value) for value in mc_dt.flatten()]\n",
    "group_percentages = ['{0:.2%}'.format(value) for value in mc_dt.flatten()/np.sum(mc_dt)]\n",
    "labels = [f'{v1}\\n{v2}\\n{v3}' for v1, v2, v3 in zip(group_names,group_counts,group_percentages)]\n",
    "labels = np.asarray(labels).reshape(2,2)\n",
    "sns.heatmap(mc_dt, annot=labels, fmt='', cmap='Blues')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exibição dos resultados\n",
    "evaluator_dt = spark.createDataFrame(\n",
    "    [(round(accuracy_dt,2), round(recall_dt,2), round(precision_dt,2), round(f1_dt,2),\\\n",
    "      int(fp_dt), int(fn_dt),\\\n",
    "      round(time_dt_train,2), round(time_dt_pred,2))],\\\n",
    "    ['acurácia','recall','precisão','f1 score',\\\n",
    "     'falso positivo', 'falso negativo',\\\n",
    "     'tempo treinamento','tempo predição'])\n",
    "print(\"Resultados do modelo Decision Tree:\")\n",
    "evaluator_dt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_dt.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_dt.createOrReplaceTempView(\"tab_result_dt\")\n",
    "dt_sql = spark.sql('select * from tab_result_dt where label != Prediction')\n",
    "dt_sql.toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest (RF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Treinamento do modelo de predição\n",
    "start_time = time.time()\n",
    "trainer = RandomForestClassifier(featuresCol='features', labelCol='label', predictionCol='prediction', probabilityCol='probability',\\\n",
    "                                 rawPredictionCol='rawPrediction', maxDepth=5, maxBins=32, minInstancesPerNode=1, minInfoGain=0.0,\\\n",
    "                                 numTrees=50, featureSubsetStrategy='auto', seed=None, subsamplingRate=1.0,\\\n",
    "                                 maxMemoryInMB=256, cacheNodeIds=False, checkpointInterval=10, impurity='gini')\n",
    "model_rf = trainer.fit(train)\n",
    "time_rf_train = time.time() - start_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execução do modelo de predição na base de teste\n",
    "start_time = time.time()\n",
    "result_rf = model_rf.transform(test)\n",
    "time_rf_pred = time.time() - start_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cálculo da acurácia do modelo de predição\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol='label', predictionCol='prediction',\\\n",
    "            metricName='accuracy')\n",
    "accuracy_rf = evaluator.evaluate(result_rf) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cálculo do recall do modelo de predição\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol='label', predictionCol='prediction',\\\n",
    "            metricName='weightedRecall')\n",
    "recall_rf = evaluator.evaluate(result_rf) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cálculo da precisão do modelo de predição\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol='label', predictionCol='prediction',\\\n",
    "            metricName='weightedPrecision')\n",
    "precision_rf = evaluator.evaluate(result_rf) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cálculo da F1 score do modelo de predição\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol='label', predictionCol='prediction',\\\n",
    "            metricName='f1')\n",
    "f1_rf = evaluator.evaluate(result_rf) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Matriz de confusão\n",
    "y_true = result_rf.select(\"label\").toPandas()\n",
    "y_pred = result_rf.select(\"prediction\").toPandas()\n",
    "mc_rf = confusion_matrix(y_true, y_pred)\n",
    "tn_rf, fp_rf, fn_rf, tp_rf = confusion_matrix(y_true, y_pred).ravel()\n",
    "print(mc_rf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "group_names = ['Verdadeiro Negativo','Falso Positivo','Falso Negativo','Verdadeiro Positivo']\n",
    "group_counts = ['{0:0.0f}'.format(value) for value in mc_dt.flatten()]\n",
    "group_percentages = ['{0:.2%}'.format(value) for value in mc_rf.flatten()/np.sum(mc_rf)]\n",
    "labels = [f'{v1}\\n{v2}\\n{v3}' for v1, v2, v3 in zip(group_names,group_counts,group_percentages)]\n",
    "labels = np.asarray(labels).reshape(2,2)\n",
    "sns.heatmap(mc_rf, annot=labels, fmt='', cmap='Blues')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exibição dos resultados\n",
    "evaluator_rf = spark.createDataFrame(\n",
    "    [(round(accuracy_rf,2), round(recall_rf,2), round(precision_rf,2), round(f1_rf,2),\\\n",
    "      int(fp_rf), int(fn_rf),\\\n",
    "      round(time_rf_train,2), round(time_rf_pred,2))],\\\n",
    "    ['acurácia','recall','precisão','f1 score',\\\n",
    "     'falso positivo', 'falso negativo',\\\n",
    "     'tempo treinamento','tempo predição'])\n",
    "print(\"Resultados do modelo Random Forest:\")\n",
    "evaluator_rf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_rf.toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Network Perceptron (NNP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Treinamento do modelo de predição\n",
    "start_time =  time.time()\n",
    "layers = [16, 5, 5, 2]\n",
    "trainer = MultilayerPerceptronClassifier(featuresCol='features', labelCol='label',\\\n",
    "          maxIter=100, layers=layers, blockSize=128, seed=1234)\n",
    "model_nnp = trainer.fit(train)\n",
    "time_nnp_train = time.time() - start_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execução do modelo de predição na base de teste\n",
    "start_time =  time.time()\n",
    "result_nnp = model_nnp.transform(test)\n",
    "time_nnp_pred = time.time() - start_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cálculo da acurácia do modelo de predição\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol='label', predictionCol='prediction',\\\n",
    "            metricName='accuracy')\n",
    "accuracy_nnp = evaluator.evaluate(result_nnp) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cálculo do recall do modelo de predição\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol='label', predictionCol='prediction',\\\n",
    "            metricName='weightedRecall')\n",
    "recall_nnp = evaluator.evaluate(result_nnp) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cálculo da precisão do modelo de predição\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol='label', predictionCol='prediction',\\\n",
    "            metricName='weightedPrecision')\n",
    "precision_nnp = evaluator.evaluate(result_nnp) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cálculo da F1 score do modelo de predição\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol='label', predictionCol='prediction',\\\n",
    "            metricName='f1')\n",
    "f1_nnp = evaluator.evaluate(result_nnp) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Matriz de confusão\n",
    "y_true = result_nnp.select(\"label\").toPandas()\n",
    "y_pred = result_nnp.select(\"prediction\").toPandas()\n",
    "mc_nnp = confusion_matrix(y_true, y_pred)\n",
    "tn_nnp, fp_nnp, fn_nnp, tp_nnp = confusion_matrix(y_true, y_pred).ravel()\n",
    "print(mc_nnp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "group_names = ['Verdadeiro Negativo','Falso Positivo','Falso Negativo','Verdadeiro Positivo']\n",
    "group_counts = ['{0:0.0f}'.format(value) for value in mc_dt.flatten()]\n",
    "group_percentages = ['{0:.2%}'.format(value) for value in mc_nnp.flatten()/np.sum(mc_nnp)]\n",
    "labels = [f'{v1}\\n{v2}\\n{v3}' for v1, v2, v3 in zip(group_names,group_counts,group_percentages)]\n",
    "labels = np.asarray(labels).reshape(2,2)\n",
    "sns.heatmap(mc_nnp, annot=labels, fmt='', cmap='Blues')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exibição dos resultados\n",
    "evaluator_nnp = spark.createDataFrame(\n",
    "    [(round(accuracy_nnp,2), round(recall_nnp,2), round(precision_nnp,2), round(f1_nnp,2),\\\n",
    "      int(fp_nnp), int(fn_nnp),\\\n",
    "      round(time_nnp_train,2), round(time_nnp_pred,2))],\\\n",
    "    ['acurácia','recall','precisão','f1 score',\\\n",
    "     'falso positivo', 'falso negativo',\\\n",
    "     'tempo treinamento','tempo predição'])\n",
    "print(\"Resultados do modelo Neural Network Perceptron:\")\n",
    "evaluator_nnp.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_nnp.toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Bayes (NB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Treinamento do modelo de predição\n",
    "start_time =  time.time()\n",
    "trainer = NaiveBayes(smoothing=1.0, modelType='multinomial')\n",
    "model_nb = trainer.fit(train)\n",
    "time_nb_train = time.time() - start_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execução do modelo de predição na base de teste\n",
    "start_time =  time.time()\n",
    "result_nb = model_nb.transform(test)\n",
    "time_nb_pred = time.time() - start_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cálculo da acurácia do modelo de predição\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol='label', predictionCol='prediction',\\\n",
    "            metricName='accuracy')\n",
    "accuracy_nb = evaluator.evaluate(result_nb) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cálculo do recall do modelo de predição\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol='label', predictionCol='prediction',\\\n",
    "            metricName='weightedRecall')\n",
    "recall_nb = evaluator.evaluate(result_nb) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cálculo da precisão do modelo de predição\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol='label', predictionCol='prediction',\\\n",
    "            metricName='weightedPrecision')\n",
    "precision_nb = evaluator.evaluate(result_nb) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cálculo da F1 score do modelo de predição\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol='label', predictionCol='prediction',\\\n",
    "            metricName='f1')\n",
    "f1_nb = evaluator.evaluate(result_nb) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Matriz de confusão\n",
    "y_true = result_nb.select(\"label\").toPandas()\n",
    "y_pred = result_nb.select(\"prediction\").toPandas()\n",
    "mc_nb = confusion_matrix(y_true, y_pred)\n",
    "tn_nb, fp_nb, fn_nb, tp_nb = confusion_matrix(y_true, y_pred).ravel()\n",
    "print(mc_nb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "group_names = ['Verdadeiro Negativo','Falso Positivo','Falso Negativo','Verdadeiro Positivo']\n",
    "group_counts = ['{0:0.0f}'.format(value) for value in mc_dt.flatten()]\n",
    "group_percentages = ['{0:.2%}'.format(value) for value in mc_nb.flatten()/np.sum(mc_nb)]\n",
    "labels = [f'{v1}\\n{v2}\\n{v3}' for v1, v2, v3 in zip(group_names,group_counts,group_percentages)]\n",
    "labels = np.asarray(labels).reshape(2,2)\n",
    "sns.heatmap(mc_nb, annot=labels, fmt='', cmap='Blues')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exibição dos resultados\n",
    "evaluator_nb = spark.createDataFrame(\n",
    "    [(round(accuracy_nb,2), round(recall_nb,2), round(precision_nb,2), round(f1_nb,2),\\\n",
    "      int(fp_nb), int(fn_nb),\\\n",
    "      round(time_nb_train,2), round(time_nb_pred,2))],\\\n",
    "    ['acurácia','recall','precisão','f1 score',\\\n",
    "     'falso positivo', 'falso negativo',\\\n",
    "     'tempo treinamento','tempo predição'])\n",
    "print(\"Resultados do modelo Naive Bayes:\")\n",
    "evaluator_nb.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_nb.toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression (LR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Treinamento do modelo de predição\n",
    "start_time =  time.time()\n",
    "trainer = LogisticRegression(maxIter=10, tol=1E-6, fitIntercept=True)\n",
    "model_lr = trainer.fit(train)\n",
    "time_lr_train = time.time() - start_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execução do modelo de predição na base de teste\n",
    "start_time =  time.time()\n",
    "result_lr = model_lr.transform(test)\n",
    "time_lr_pred = time.time() - start_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cálculo da acurácia do modelo de predição\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol='label', predictionCol='prediction',\\\n",
    "            metricName='accuracy')\n",
    "accuracy_lr = evaluator.evaluate(result_lr) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cálculo do recall do modelo de predição\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol='label', predictionCol='prediction',\\\n",
    "            metricName='weightedRecall')\n",
    "recall_lr = evaluator.evaluate(result_lr) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cálculo da precisão do modelo de predição\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol='label', predictionCol='prediction',\\\n",
    "            metricName='weightedPrecision')\n",
    "precision_lr = evaluator.evaluate(result_lr) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cálculo da F1 score do modelo de predição\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol='label', predictionCol='prediction',\\\n",
    "            metricName='f1')\n",
    "f1_lr = evaluator.evaluate(result_lr) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Matriz de confusão\n",
    "y_true = result_lr.select(\"label\").toPandas()\n",
    "y_pred = result_lr.select(\"prediction\").toPandas()\n",
    "mc_lr = confusion_matrix(y_true, y_pred)\n",
    "tn_lr, fp_lr, fn_lr, tp_lr = confusion_matrix(y_true, y_pred).ravel()\n",
    "print(mc_lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "group_names = ['Verdadeiro Negativo','Falso Positivo','Falso Negativo','Verdadeiro Positivo']\n",
    "group_counts = ['{0:0.0f}'.format(value) for value in mc_dt.flatten()]\n",
    "group_percentages = ['{0:.2%}'.format(value) for value in mc_lr.flatten()/np.sum(mc_lr)]\n",
    "labels = [f'{v1}\\n{v2}\\n{v3}' for v1, v2, v3 in zip(group_names,group_counts,group_percentages)]\n",
    "labels = np.asarray(labels).reshape(2,2)\n",
    "sns.heatmap(mc_lr, annot=labels, fmt='', cmap='Blues')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exibição dos resultados\n",
    "evaluator_lr = spark.createDataFrame(\n",
    "    [(round(accuracy_lr,2), round(recall_lr,2), round(precision_lr,2), round(f1_lr,2),\\\n",
    "      int(fp_lr), int(fn_lr),\\\n",
    "      round(time_lr_train,2), round(time_lr_pred,2))],\\\n",
    "    ['acurácia','recall','precisão','f1 score',\\\n",
    "     'falso positivo', 'falso negativo',\\\n",
    "     'tempo treinamento','tempo predição'])\n",
    "print(\"Resultados do modelo Logistic Regression:\")\n",
    "evaluator_lr.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_lr.toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Suport Vector Machines (SVM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Treinamento do modelo de predição\n",
    "start_time =  time.time()\n",
    "trainer = LinearSVC(featuresCol='features', labelCol='label',\\\n",
    "                    maxIter=100, regParam=0.1)\n",
    "model_svm = trainer.fit(train)\n",
    "time_svm_train = time.time() - start_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execução do modelo de predição na base de teste\n",
    "start_time =  time.time()\n",
    "result_svm = model_svm.transform(test)\n",
    "time_svm_pred = time.time() - start_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cálculo da acurácia do modelo de predição\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol='label', predictionCol='prediction',\\\n",
    "            metricName='accuracy')\n",
    "accuracy_svm = evaluator.evaluate(result_svm) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cálculo do recall do modelo de predição\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol='label', predictionCol='prediction',\\\n",
    "            metricName='weightedRecall')\n",
    "recall_svm = evaluator.evaluate(result_svm) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cálculo da precisão do modelo de predição\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol='label', predictionCol='prediction',\\\n",
    "            metricName='weightedPrecision')\n",
    "precision_svm = evaluator.evaluate(result_svm) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cálculo da F1 score do modelo de predição\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol='label', predictionCol='prediction',\\\n",
    "            metricName='f1')\n",
    "f1_svm = evaluator.evaluate(result_svm) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Matriz de confusão\n",
    "y_true = result_svm.select(\"label\").toPandas()\n",
    "y_pred = result_svm.select(\"prediction\").toPandas()\n",
    "mc_svm = confusion_matrix(y_true, y_pred)\n",
    "tn_svm, fp_svm, fn_svm, tp_svm = confusion_matrix(y_true, y_pred).ravel()\n",
    "print(mc_svm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "group_names = ['Verdadeiro Negativo','Falso Positivo','Falso Negativo','Verdadeiro Positivo']\n",
    "group_counts = ['{0:0.0f}'.format(value) for value in mc_dt.flatten()]\n",
    "group_percentages = ['{0:.2%}'.format(value) for value in mc_svm.flatten()/np.sum(mc_svm)]\n",
    "labels = [f'{v1}\\n{v2}\\n{v3}' for v1, v2, v3 in zip(group_names,group_counts,group_percentages)]\n",
    "labels = np.asarray(labels).reshape(2,2)\n",
    "sns.heatmap(mc_svm, annot=labels, fmt='', cmap='Blues')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exibição dos resultados\n",
    "evaluator_svm = spark.createDataFrame(\n",
    "    [(round(accuracy_svm,2), round(recall_svm,2), round(precision_svm,2), round(f1_svm,2),\\\n",
    "      int(fp_svm), int(fn_svm),\\\n",
    "      round(time_svm_train,2), round(time_svm_pred,2))],\\\n",
    "    ['acurácia','recall','precisão','f1 score',\\\n",
    "     'falso positivo', 'falso negativo',\\\n",
    "     'tempo treinamento','tempo predição'])\n",
    "print(\"Resultados do modelo Suport Vector Machines:\")\n",
    "evaluator_svm.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_svm.toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resultados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = ['Decision Tree','Random Forest','Neural Network Perceptron','Naive Bayes','Logistic Regression','Suport Vector Machines']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Ranking Accuracy %')\n",
    "\n",
    "list = ((models[0],accuracy_dt),\\\n",
    "        (models[1],accuracy_rf),\\\n",
    "        (models[2],accuracy_nnp),\\\n",
    "        (models[3],accuracy_nb),\\\n",
    "        (models[4],accuracy_lr),\\\n",
    "        (models[5],accuracy_nb))\n",
    "df_acuracia = spark.createDataFrame(list, ['Modelo', 'Acuracia'])\n",
    "df_acuracia.sort(df_acuracia.Acuracia.desc()).toPandas()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Ranking')\n",
    "\n",
    "list = ((models[0],recall_dt),\\\n",
    "        (models[1],recall_rf),\\\n",
    "        (models[2],recall_nnp),\\\n",
    "        (models[3],recall_nb),\\\n",
    "        (models[4],recall_lr),\\\n",
    "        (models[5],recall_svm))\n",
    "df_recall = spark.createDataFrame(list, ['Modelo', 'Recall'])\n",
    "df_recall.sort(df_recall.Recall.desc()).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Ranking Precision %')\n",
    "\n",
    "list = ((models[0],precision_dt),\\\n",
    "        (models[1],precision_rf),\\\n",
    "        (models[2],precision_nnp),\\\n",
    "        (models[3],precision_nb),\\\n",
    "        (models[4],precision_lr),\\\n",
    "        (models[5],precision_svm))\n",
    "df_precision = spark.createDataFrame(list, ['Modelo', 'Precisao'])\n",
    "df_precision.sort(df_precision.Precisao.desc()).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### F1 score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list = ((models[0],f1_dt),\\\n",
    "        (models[1],f1_rf),\\\n",
    "        (models[2],f1_nnp),\\\n",
    "        (models[3],f1_nb),\\\n",
    "        (models[4],f1_lr),\\\n",
    "        (models[5],f1_svm))\n",
    "df_f1 = spark.createDataFrame(list, ['Modelo', 'F1'])\n",
    "df_f1.sort(df_f1.F1.desc()).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tempo de Treinamento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list = ((models[0],time_dt_train),\\\n",
    "        (models[1],time_rf_train),\\\n",
    "        (models[2],time_nnp_train),\\\n",
    "        (models[3],time_nb_train),\\\n",
    "        (models[4],time_lr_train),\\\n",
    "        (models[5],time_svm_train))\n",
    "df_time_train = spark.createDataFrame(list, ['Modelo', 'Tempo_Treinamento'])\n",
    "df_time_train.sort(df_time_train.Tempo_Treinamento.asc()).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tempo de Predição"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list = ((models[0],time_dt_pred),\\\n",
    "        (models[1],time_rf_pred),\\\n",
    "        (models[2],time_nnp_pred),\\\n",
    "        (models[3],time_nnp_pred),\\\n",
    "        (models[4],time_nnp_pred),\\\n",
    "        (models[5],time_nnp_pred))\n",
    "df_time_pred = spark.createDataFrame(list, ['Modelo', 'Tempo_Predicao'])\n",
    "df_time_pred.sort(df_time_pred.Tempo_Predicao.asc()).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Falso Positivo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list = ((models[0],int(fp_dt)),\\\n",
    "        (models[1],int(fp_rf)),\\\n",
    "        (models[2],int(fp_nnp)),\\\n",
    "        (models[3],int(fp_nb)),\\\n",
    "        (models[4],int(fp_lr)),\\\n",
    "        (models[5],int(fp_svm)))\n",
    "df_fp = spark.createDataFrame(list, ['Modelo', 'Falso_Positivo'])\n",
    "df_fp.sort(df_fp.Falso_Positivo.asc()).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Falso Negativo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list = ((models[0],int(fn_dt)),\\\n",
    "        (models[1],int(fn_rf)),\\\n",
    "        (models[2],int(fn_nnp)),\\\n",
    "        (models[3],int(fn_nb)),\\\n",
    "        (models[4],int(fn_lr)),\\\n",
    "        (models[5], int(fn_svm)))\n",
    "df_fn = spark.createDataFrame(list, ['Modelo', 'Falso_Negativo'])\n",
    "df_fn.sort(df_fn.Falso_Negativo.asc()).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparativo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_acuracia.join(df_fp, \"Modelo\")\n",
    "df = df.join(df_fn, \"Modelo\")\n",
    "df = df.join(df_time_train, \"Modelo\")\n",
    "df = df.join(df_time_pred, \"Modelo\")\n",
    "df.sort(df.Acuracia.desc()).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exportação dos modelos para o disco"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modelo Decision Tree\n",
    "model_dt.save('model_dt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modelo Random Forest\n",
    "model_rf.save('model_rf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modelo Neural Network Perceptron\n",
    "model_nnp.save('model_nnp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modelo Suport Vector Machines\n",
    "model_svm.save('model_svm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modelo Naive Bayes\n",
    "model_nb.save('model_nb')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modelo Logistic Regression\n",
    "model_lr.save('model_lr')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
